{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "import numpy as np\n",
    "import mne\n",
    "\n",
    "%matplotlib widget"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Loading Data\n",
    "\n",
    "MNE-Python data-structures are based around `FIF file format` from `Neuromag`.\n",
    "\n",
    "EEG and MEG data from a subject performing an audiovisual experiment, along with MRI scans for that subject"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "sample_data_folder = mne.datasets.sample.data_path()\n",
    "sample_data_raw_file = os.path.join(\n",
    "    sample_data_folder,\n",
    "    'MEG',\n",
    "    'sample',\n",
    "    'sample_audvis_filt-0-40_raw.fif')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Opening raw data file /Users/markyousef/mne_data/MNE-sample-data/MEG/sample/sample_audvis_filt-0-40_raw.fif...\n",
      "    Read a total of 4 projection items:\n",
      "        PCA-v1 (1 x 102)  idle\n",
      "        PCA-v2 (1 x 102)  idle\n",
      "        PCA-v3 (1 x 102)  idle\n",
      "        Average EEG reference (1 x 60)  idle\n",
      "    Range : 6450 ... 48149 =     42.956 ...   320.665 secs\n",
      "Ready.\n",
      "Current compensation grade : 0\n"
     ]
    }
   ],
   "source": [
    "raw = mne.io.read_raw_fif(sample_data_raw_file)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**SSP Projector:**\n",
    "A projector (abbr. proj), also referred to as Signal Space Projection (SSP), defines a linear operation applied spatially to EEG or MEG data. You can see this as a matrix multiplication that reduces the rank of the data by projecting it to a lower dimensional subspace. Such a projection operator is applied to both the data and the forward operator for source localization. Note that EEG average referencing can be done using such a projection operator. It is stored in the measurement info in info['projs'].\n",
    "\n",
    "[Background on projectors and projections](https://mne.tools/dev/auto_tutorials/intro/plot_40_projectors_background.html#tut-projectors-background)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "<Raw  |  sample_audvis_filt-0-40_raw.fif, n_channels x n_times : 376 x 41700 (277.7 sec), ~3.6 MB, data not loaded>\n"
     ]
    }
   ],
   "source": [
    "print(raw)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "<Info | 19 non-empty fields\n",
      "    bads : list | MEG 2443, EEG 053\n",
      "    ch_names : list | MEG 0113, MEG 0112, MEG 0111, MEG 0122, MEG 0123, ...\n",
      "    chs : list | 376 items (GRAD: 204, MAG: 102, STIM: 9, EEG: 60, EOG: 1)\n",
      "    comps : list | 0 items\n",
      "    custom_ref_applied : bool | False\n",
      "    dev_head_t : Transform | 3 items\n",
      "    dig : list | 146 items (3 Cardinal, 4 HPI, 61 EEG, 78 Extra)\n",
      "    events : list | 0 items\n",
      "    file_id : dict | 4 items\n",
      "    highpass : float | 0.10000000149011612 Hz\n",
      "    hpi_meas : list | 1 items\n",
      "    hpi_results : list | 1 items\n",
      "    lowpass : float | 40.0 Hz\n",
      "    meas_date : tuple | 2002-12-03 19:01:10 GMT\n",
      "    meas_id : dict | 4 items\n",
      "    nchan : int | 376\n",
      "    proc_history : list | 0 items\n",
      "    projs : list | PCA-v1: off, PCA-v2: off, PCA-v3: off, ...\n",
      "    sfreq : float | 150.15374755859375 Hz\n",
      "    acq_pars : NoneType\n",
      "    acq_stim : NoneType\n",
      "    ctf_head_t : NoneType\n",
      "    description : NoneType\n",
      "    dev_ctf_t : NoneType\n",
      "    experimenter : NoneType\n",
      "    gantry_angle : NoneType\n",
      "    hpi_subsystem : NoneType\n",
      "    kit_system_id : NoneType\n",
      "    line_freq : NoneType\n",
      "    proj_id : NoneType\n",
      "    proj_name : NoneType\n",
      "    subject_info : NoneType\n",
      "    xplotter_layout : NoneType\n",
      ">\n"
     ]
    }
   ],
   "source": [
    "print(raw.info)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Power Spectral Density (PSD) and raw traces for each sensor.\n",
    "For the PSD plot, we'll only plot frequencies below 50 Hz – data is low-pass filtered at 40 Hz."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "193d12a0e8c24b609a161ec55bd964cc",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Canvas(toolbar=Toolbar(toolitems=[('Home', 'Reset original view', 'home', 'home'), ('Back', 'Back to previous …"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Effective window size : 13.639 (s)\n",
      "Effective window size : 13.639 (s)\n",
      "Effective window size : 13.639 (s)\n"
     ]
    }
   ],
   "source": [
    "raw.plot_psd(fmax=50);"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "6b769689e5ab47d4b80ab3f13758d0aa",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Canvas(toolbar=Toolbar(toolitems=[('Home', 'Reset original view', 'home', 'home'), ('Back', 'Back to previous …"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "raw.plot(duration=5, n_channels=30);"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Preprocessing\n",
    "\n",
    "Clean up data by performing independent component analysis (ICA). We'll skip the steps that helped determine which components that best capture the artifacts.\n",
    "[Repairing artifacts with ICA](https://mne.tools/dev/auto_tutorials/preprocessing/plot_40_artifact_correction_ica.html#tut-artifact-ica)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Fitting ICA to data using 364 channels (please be patient, this may take a while)\n",
      "Inferring max_pca_components from picks\n",
      "Selection by number: 20 components\n",
      "Fitting ICA took 3.8s.\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "<ICA  |  raw data decomposition, fit (fastica): 41700 samples, 20 components, channels used: \"mag\"; \"grad\"; \"eeg\">"
      ]
     },
     "execution_count": 8,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "ica = mne.preprocessing.ICA(n_components=20, random_state=97, max_iter=800)\n",
    "ica.fit(raw)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "ica.exclude = [1,2]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "a00c7ebf787548c39f7c9caaf03bc183",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Canvas(toolbar=Toolbar(toolitems=[('Home', 'Reset original view', 'home', 'home'), ('Back', 'Back to previous …"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "    Using multitaper spectrum estimation with 7 DPSS windows\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "ae18d9af312e4647a486228be91f1dae",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Canvas(toolbar=Toolbar(toolitems=[('Home', 'Reset original view', 'home', 'home'), ('Back', 'Back to previous …"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "ica.plot_properties(raw, picks=ica.exclude);"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Reading 0 ... 41699  =      0.000 ...   277.709 secs...\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "<Raw  |  sample_audvis_filt-0-40_raw.fif, n_channels x n_times : 376 x 41700 (277.7 sec), ~123.3 MB, data loaded>"
      ]
     },
     "execution_count": 11,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "orig_raw = raw.copy()\n",
    "raw.load_data()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Transforming to ICA space (20 components)\n",
      "Zeroing out 2 ICA components\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "<Raw  |  sample_audvis_filt-0-40_raw.fif, n_channels x n_times : 376 x 41700 (277.7 sec), ~123.3 MB, data loaded>"
      ]
     },
     "execution_count": 12,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "ica.apply(raw)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Show some frontal channels to clearly illustrate the artifact removal"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "377bdb4e78eb4a4fac6d36ddceeb17b5",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Canvas(toolbar=Toolbar(toolitems=[('Home', 'Reset original view', 'home', 'home'), ('Back', 'Back to previous …"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "4e7a2de48d164f15af135bf033be840a",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Canvas(toolbar=Toolbar(toolitems=[('Home', 'Reset original view', 'home', 'home'), ('Back', 'Back to previous …"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "chs = ['MEG 0111', 'MEG 0121', 'MEG 0131', 'MEG 0211', 'MEG 0221', 'MEG 0231',\n",
    "       'MEG 0311', 'MEG 0321', 'MEG 0331', 'MEG 1511', 'MEG 1521', 'MEG 1531',\n",
    "       'EEG 001', 'EEG 002', 'EEG 003', 'EEG 004', 'EEG 005', 'EEG 006',\n",
    "       'EEG 007', 'EEG 008']\n",
    "\n",
    "chan_idxs = [raw.ch_names.index(ch) for ch in chs]\n",
    "orig_raw.plot(order=chan_idxs, start=12, duration=4)\n",
    "raw.plot(order=chan_idxs, start=12, duration=4);"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Detecting experimental events"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "319 events found\n",
      "Event IDs: [ 1  2  3  4  5 32]\n"
     ]
    }
   ],
   "source": [
    "events = mne.find_events(raw, stim_channel='STI 014')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "0: sample number\n",
    "1: often ignored\n",
    "2: event ID"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[[6994    0    2]\n",
      " [7086    0    3]\n",
      " [7192    0    1]\n",
      " [7304    0    4]\n",
      " [7413    0    2]]\n"
     ]
    }
   ],
   "source": [
    "print(events[:5])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [],
   "source": [
    "event_dict = {'auditory/left': 1, 'auditory/right': 2, 'visual/left': 3, 'visual/right': 4, 'smiley': 5, 'buttonpress': 32}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "33130a2f1cd2475aa5968e33d583f371",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Canvas(toolbar=Toolbar(toolitems=[('Home', 'Reset original view', 'home', 'home'), ('Back', 'Back to previous …"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "fig = mne.viz.plot_events(events, event_id=event_dict, sfreq=raw.info['sfreq']);"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Epoching continuous data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [],
   "source": [
    "reject_criteria = dict(mag=4000e-15, # 4000 fT\n",
    "                      grad=4000e-13, # 4000 fT/cm\n",
    "                      eeg=150e-6, # 150 uV\n",
    "                      eog=250e-6 # 250 uV\n",
    "                      )"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "319 matching events found\n",
      "Applying baseline correction (mode: mean)\n",
      "Not setting metadata\n",
      "Created an SSP operator (subspace dimension = 4)\n",
      "4 projection items activated\n",
      "Loading data for 319 events and 106 original time points ...\n",
      "    Rejecting  epoch based on EOG : ['EOG 061']\n",
      "    Rejecting  epoch based on EOG : ['EOG 061']\n",
      "    Rejecting  epoch based on MAG : ['MEG 1711']\n",
      "    Rejecting  epoch based on EOG : ['EOG 061']\n",
      "    Rejecting  epoch based on EOG : ['EOG 061']\n",
      "    Rejecting  epoch based on MAG : ['MEG 1711']\n",
      "    Rejecting  epoch based on EEG : ['EEG 008']\n",
      "    Rejecting  epoch based on EOG : ['EOG 061']\n",
      "    Rejecting  epoch based on EOG : ['EOG 061']\n",
      "    Rejecting  epoch based on EOG : ['EOG 061']\n",
      "10 bad epochs dropped\n"
     ]
    }
   ],
   "source": [
    "epochs = mne.Epochs(raw, events, event_id=event_dict, tmin=-0.2, tmax=0.5, reject=reject_criteria, preload=True)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Pool across left/right stimulus presentations so we can compare auditory vs. visual responses. To avoid biasing signals to the left or right, we'll `equalize_event_counts` to randomly sample epochs from each condition to match the number of epochs present in the condition with the fewest good epochs."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {},
   "outputs": [],
   "source": [
    "conds_we_care_about = ['auditory/left', 'auditory/right', 'visual/left', 'visual/right']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Dropped 7 epochs\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "(<Epochs  |   302 events (all good), -0.199795 - 0.499488 sec, baseline [None, 0], ~95.5 MB, data loaded,\n",
       "  'auditory/left': 68\n",
       "  'auditory/right': 68\n",
       "  'buttonpress': 16\n",
       "  'smiley': 14\n",
       "  'visual/left': 68\n",
       "  'visual/right': 68>, array([273, 275, 121, 195, 258, 271, 274]))"
      ]
     },
     "execution_count": 21,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "epochs.equalize_event_counts(conds_we_care_about) # this operates in-place"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {},
   "outputs": [],
   "source": [
    "aud_epochs = epochs['auditory']\n",
    "vis_epochs = epochs['visual']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {},
   "outputs": [],
   "source": [
    "del raw, epochs # free up memory"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Show each epoch as one row of an image map, with color representing signal magnitude; the avg. evoked response and the sensor location are shown below the image"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "b935645b747f43ccba0f63ed9350286c",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Canvas(toolbar=Toolbar(toolitems=[('Home', 'Reset original view', 'home', 'home'), ('Back', 'Back to previous …"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "a5ca44d66a034247bee235d24c432d93",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Canvas(toolbar=Toolbar(toolitems=[('Home', 'Reset original view', 'home', 'home'), ('Back', 'Back to previous …"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "aud_epochs.plot_image(picks=['MEG 1332', 'EEG 021']);"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Time-frequency analysis\n",
    "\n",
    "* time-frequency representations\n",
    "* power spectral density\n",
    "* cross-spectral density"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "For the auditory epochs compute the induced power at different frequencies  and times using `Morlet wavelets`"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "metadata": {},
   "outputs": [],
   "source": [
    "frequencies = np.arange(7, 30, 3)\n",
    "power = mne.time_frequency.tfr_morlet(aud_epochs, n_cycles=2, return_itc=False, freqs=frequencies, decim=3)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "No baseline correction applied\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "f3f57e177b4c4b728713b587e1bdd987",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Canvas(toolbar=Toolbar(toolitems=[('Home', 'Reset original view', 'home', 'home'), ('Back', 'Back to previous …"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "power.plot(['MEG 1332']);"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Estimating evoked responses"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Get estimate of evoked responses to auditory versus visual stimuli by averaging together the epochs in each condition, "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "metadata": {},
   "outputs": [],
   "source": [
    "aud_evoked = aud_epochs.average()\n",
    "vis_evoked = vis_epochs.average()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "No picks, plotting the GFP ...\n",
      "Multiple channel types selected, returning one figure per type.\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "40b28cb88ba240c5880ad5d24f79f890",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Canvas(toolbar=Toolbar(toolitems=[('Home', 'Reset original view', 'home', 'home'), ('Back', 'Back to previous …"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "787da4ce795d45bab937376f1413ada1",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Canvas(toolbar=Toolbar(toolitems=[('Home', 'Reset original view', 'home', 'home'), ('Back', 'Back to previous …"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "a24eaddef1eb47e0a41db3eba4d29cd4",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Canvas(toolbar=Toolbar(toolitems=[('Home', 'Reset original view', 'home', 'home'), ('Back', 'Back to previous …"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Combining all planar gradiometers with RMSE.\n"
     ]
    }
   ],
   "source": [
    "mne.viz.plot_compare_evokeds(dict(auditory=aud_evoked, visual=vis_evoked), show_legend='upper left', show_sensors='upper right');"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Examining just the EEG channels, we'll see the classic auditory evoked N100-P200 pattern over dorso-frontal electrodes, the plot scalp topographies at some additional arbitrary times:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "b709fce9b4a540c59732e43377a830a8",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Canvas(toolbar=Toolbar(toolitems=[('Home', 'Reset original view', 'home', 'home'), ('Back', 'Back to previous …"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "aud_evoked.plot_joint(picks='eeg');"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "0888be4de0c64c439060baf87e8c3f84",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Canvas(toolbar=Toolbar(toolitems=[('Home', 'Reset original view', 'home', 'home'), ('Back', 'Back to previous …"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "aud_evoked.plot_topomap(times=[0., 0.08, 0.1, 0.12, 0.2], ch_type='eeg');"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Combining evoked responses to show contrast betwen conditions. Plot the difference wave at each sensor:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 31,
   "metadata": {},
   "outputs": [],
   "source": [
    "evoked_diff = mne.combine_evoked([aud_evoked, -vis_evoked], weights='equal')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 32,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "48f1faff0bac4c4fa7da5acf057981e0",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Canvas(toolbar=Toolbar(toolitems=[('Home', 'Reset original view', 'home', 'home'), ('Back', 'Back to previous …"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "evoked_diff.pick_types('mag').plot_topo(color='r', legend=False);"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Inverse modeling"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Estimating the origins of the evoked activity by projecting the sensor data into the subject's `source space`. Here we'll use a minimum-norm estimation (MNE) to generate a continous map of activation contrained to the cortical surface. \n",
    "MNE uses a linear `inverse operator` to project EEG+MEG sensor measurements into the source space. The inverse operator is computed from the `forward solution` for this subject and estimate of `the covariance of sensor measurements`"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 33,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Reading inverse operator decomposition from /Users/markyousef/mne_data/MNE-sample-data/MEG/sample/sample_audvis-meg-oct-6-meg-inv.fif...\n",
      "    Reading inverse operator info...\n",
      "    [done]\n",
      "    Reading inverse operator decomposition...\n",
      "    [done]\n",
      "    305 x 305 full covariance (kind = 1) found.\n",
      "    Read a total of 4 projection items:\n",
      "        PCA-v1 (1 x 102) active\n",
      "        PCA-v2 (1 x 102) active\n",
      "        PCA-v3 (1 x 102) active\n",
      "        Average EEG reference (1 x 60) active\n",
      "    Noise covariance matrix read.\n",
      "    22494 x 22494 diagonal covariance (kind = 2) found.\n",
      "    Source covariance matrix read.\n",
      "    22494 x 22494 diagonal covariance (kind = 6) found.\n",
      "    Orientation priors read.\n",
      "    22494 x 22494 diagonal covariance (kind = 5) found.\n",
      "    Depth priors read.\n",
      "    Did not find the desired covariance matrix (kind = 3)\n",
      "    Reading a source space...\n",
      "    Computing patch statistics...\n",
      "    Patch information added...\n",
      "    Distance information added...\n",
      "    [done]\n",
      "    Reading a source space...\n",
      "    Computing patch statistics...\n",
      "    Patch information added...\n",
      "    Distance information added...\n",
      "    [done]\n",
      "    2 source spaces read\n",
      "    Read a total of 4 projection items:\n",
      "        PCA-v1 (1 x 102) active\n",
      "        PCA-v2 (1 x 102) active\n",
      "        PCA-v3 (1 x 102) active\n",
      "        Average EEG reference (1 x 60) active\n",
      "    Source spaces transformed to the inverse solution coordinate frame\n",
      "Preparing the inverse operator for use...\n",
      "    Scaled noise and source covariance from nave = 1 to nave = 136\n",
      "    Created the regularized inverter\n",
      "    Created an SSP operator (subspace dimension = 3)\n",
      "    Created the whitener using a noise covariance matrix with rank 302 (3 small eigenvalues omitted)\n",
      "Applying inverse operator to \"0.50 * visual/left + 0.50 * visual/right\"...\n",
      "    Picked 305 channels from the data\n",
      "    Computing inverse...\n",
      "    Eigenleads need to be weighted ...\n",
      "    Computing residual...\n",
      "    Explained  70.2% variance\n",
      "    Combining the current components...\n",
      "[done]\n"
     ]
    }
   ],
   "source": [
    "# load inverse operator\n",
    "inverse_operator_file = os.path.join(sample_data_folder, 'MEG', 'sample', 'sample_audvis-meg-oct-6-meg-inv.fif')\n",
    "inverse_operator = mne.minimum_norm.read_inverse_operator(inverse_operator_file)\n",
    "\n",
    "# set SNR to compute regularization parameter (λ²)\n",
    "snr = 3.\n",
    "lambda2 = 1. / snr ** 2\n",
    "\n",
    "# generate the source time course (STC)\n",
    "stc = mne.minimum_norm.apply_inverse(vis_evoked, inverse_operator, lambda2=lambda2, method='MNE')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 34,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Using control points [8.61922423e-11 1.06837855e-10 4.49139511e-10]\n"
     ]
    }
   ],
   "source": [
    "# path to subjects' MRI files\n",
    "subjects_dir = os.path.join(sample_data_folder, 'subjects')\n",
    "stc.plot(initial_time=0.1, hemi='split', views=['lat', 'med'], subjects_dir=subjects_dir);"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.4"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
